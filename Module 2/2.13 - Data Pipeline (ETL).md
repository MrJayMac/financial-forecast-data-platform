### Data Movement Approaches: ETL, ELT, and Data Pipelines

---

#### ETL (Extract, Transform, Load)
- **Purpose:** Convert raw data into analysis-ready form.
- **Steps:**
  1. **Extract:** Collect from sources.  
     - *Batch processing* (chunks at intervals) → Tools: Stitch, Blendo.  
     - *Stream processing* (real-time) → Tools: Apache Samza, Storm, Kafka.  
  2. **Transform:** Clean, standardize, enrich, validate, and apply business rules.  
  3. **Load:** Move into target system.  
     - Initial load, incremental updates, or full refresh.  
     - Includes verification, monitoring, and recovery.  
- **Use Cases:** Historically batch, now also real-time with streaming ETL.  
- **Popular Tools:** IBM Infosphere, AWS Glue, Improvado, Skyvia, HEVO, Informatica PowerCenter.

---

#### ELT (Extract, Load, Transform)
- **Process:** Extract → Load raw data into target → Transform inside target system.  
- **Destination:** Typically data lakes, sometimes data warehouses.  
- **Advantages:**  
  - Faster cycle (data loaded immediately).  
  - Ideal for large, unstructured/non-relational data.  
  - Greater flexibility for exploratory analysis.  
  - Only transform data needed for specific use cases.  
  - Better suited for Big Data.  

---

#### Data Pipelines
- **Definition:** Broader term for moving data end-to-end (ETL/ELT are subsets).  
- **Types:** Batch, streaming, or hybrid.  
- **Streaming Example:** Continuous updates (e.g., traffic sensor data).  
- **Features:** High performance, supports both long-running batch and interactive queries.  
- **Destinations:** Data lakes, apps, visualization tools.  
- **Popular Tools:** Apache Beam, Apache Airflow, Google Dataflow.  

---

**Summary**  
- **ETL:** Transform before loading → analysis-ready data.  
- **ELT:** Load raw data first, then transform → flexible, Big Data friendly.  
- **Data Pipelines:** Full end-to-end framework, can use ETL/ELT as parts.  
